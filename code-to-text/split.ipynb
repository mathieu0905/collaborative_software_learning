{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3372eb9dbea7bd68",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad9e0e1f865453",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T12:47:19.967835200Z",
     "start_time": "2023-12-23T12:47:14.490307900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def process_file(file_path, label):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "                    # item['language'] = label\n",
    "                    data.append(item)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON in line: {e}\")\n",
    "        return data\n",
    "    except IOError as e:\n",
    "        print(f\"Error opening file: {file_path}, {e}\")\n",
    "\n",
    "def process_directory(directory):\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        label = os.path.basename(subdir)\n",
    "        for file in files:\n",
    "            if file.endswith('.jsonl'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                data_partial = process_file(file_path, label)\n",
    "                data.extend(data_partial)\n",
    "                all_data.update({label: data_partial})\n",
    "\n",
    "data = []\n",
    "all_data = {}\n",
    "directory_path = 'dataset'\n",
    "process_directory(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff5d12adabb435",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T12:22:27.855150300Z",
     "start_time": "2023-12-23T12:22:27.840143500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(all_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd27d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in all_data.values():\n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c962b846eb18c3d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 均匀划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47d6b1b6597f54",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# data = [...]\n",
    "\n",
    "def stratified_group_split(data, n_splits=10):\n",
    "    labels = [d['language'] for d in data]\n",
    "    features = [i for i in range(len(data))]\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=1/n_splits)\n",
    "    \n",
    "    grouped_data = defaultdict(list)\n",
    "    \n",
    "    for group_index, (_, test_index) in enumerate(sss.split(features, labels)):\n",
    "        for i in test_index:\n",
    "            grouped_data[group_index].append(data[i])\n",
    "    \n",
    "    return grouped_data\n",
    "\n",
    "def save_group_data(data, index, directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    group_file = os.path.join(directory, f'group_{index}.json')\n",
    "    with open(group_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(group_data, f, indent=4)\n",
    "\n",
    "def count_languages(grouped_data):\n",
    "    for group_index, items in grouped_data.items():\n",
    "        language_count = defaultdict(int)\n",
    "        for item in items:\n",
    "            language = item.get('language', 'Unknown')\n",
    "            language_count[language] += 1\n",
    "        print(f\"Group {group_index}:\")\n",
    "        for language, count in language_count.items():\n",
    "            print(f\"    {language}: {count}\")\n",
    "\n",
    "grouped_data = stratified_group_split(data)\n",
    "\n",
    "count_languages(grouped_data)\n",
    "\n",
    "for index, group_data in grouped_data.items():\n",
    "    save_group_data(group_data, index, 'lans/splits')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2bb1c99c89071a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 不均匀划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fadccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in all_data.items():\n",
    "    print(f'{key}: {len(value)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dcda61681a7306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T11:46:33.179626900Z",
     "start_time": "2023-12-23T11:46:33.170101500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf975272e2a3d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T11:46:46.894858700Z",
     "start_time": "2023-12-23T11:46:46.796526Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "all_data_num = [len(value) for value in all_data.values()]\n",
    "keys = list(all_data.keys())\n",
    "\n",
    "def generate_balanced_matrix(rows, cols, row_sum, col_sums):\n",
    "    if sum(col_sums) < rows * row_sum or sum(col_sums) > rows * (row_sum + 1):\n",
    "        return None\n",
    "\n",
    "    matrix = np.zeros((rows, cols), dtype=int)\n",
    "\n",
    "    for col in range(cols):\n",
    "        col_sum_remaining = col_sums[col]\n",
    "\n",
    "        while col_sum_remaining > 0:\n",
    "            rows_with_space = np.where(np.sum(matrix, axis=1) < row_sum + 1)[0]\n",
    "            if len(rows_with_space) == 0:\n",
    "                break\n",
    "\n",
    "            row = np.random.choice(rows_with_space)\n",
    "            max_possible_addition = min(col_sum_remaining, row_sum + 1 - np.sum(matrix[row]))\n",
    "            addition = np.random.randint(1, max_possible_addition + 1)\n",
    "            matrix[row, col] += addition\n",
    "            col_sum_remaining -= addition\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def redistribute_values_evenly(matrix, redistribute_limit=50):\n",
    "    rows, cols = matrix.shape\n",
    "\n",
    "    for col in range(cols):\n",
    "        for _ in range(redistribute_limit):\n",
    "            max_row = np.argmax(matrix[:, col])\n",
    "            min_row = np.argmin(matrix[:, col])\n",
    "            avg_value = np.mean(matrix[:, col])\n",
    "\n",
    "            if matrix[max_row, col] > avg_value + 30:\n",
    "                matrix[max_row, col] -= 1\n",
    "                matrix[min_row, col] += 1\n",
    "\n",
    "    return matrix\n",
    "\n",
    "rows = 10\n",
    "cols = len(all_data_num)\n",
    "total_count = sum(all_data_num)\n",
    "row_sum = total_count // rows\n",
    "\n",
    "balanced_matrix = generate_balanced_matrix(rows, cols, row_sum, all_data_num)\n",
    "\n",
    "if balanced_matrix is not None:\n",
    "    redistributed_matrix = redistribute_values_evenly(balanced_matrix)\n",
    "\n",
    "    for i, row in enumerate(redistributed_matrix):\n",
    "        group_data = {lang: value for lang, value in zip(keys, row)}\n",
    "        group_total = sum(row)\n",
    "        print(f\"Group {i+1}: {group_data}, Number: {group_total}\")\n",
    "\n",
    "    total_outputs = {lang: 0 for lang in keys}\n",
    "    for row in redistributed_matrix:\n",
    "        for lang, value in zip(keys, row):\n",
    "            total_outputs[lang] += value\n",
    "\n",
    "    print(\"\\n\")\n",
    "    for lang, total in total_outputs.items():\n",
    "        print(f\"{lang}: {total}\")\n",
    "else:\n",
    "    print(\"cannot generate balanced matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ecc22a600e1b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T11:55:20.747054800Z",
     "start_time": "2023-12-23T11:55:20.658236600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "def divide_data_according_to_matrix(data, matrix):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param data: A dictionary containing the data to be divided.\n",
    "    :param matrix: A 2D numpy array containing the number of items to be placed in each group.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    data_copy = copy.deepcopy(data)\n",
    "\n",
    "    os.makedirs('lans/splits_uneven', exist_ok=True)\n",
    "    for i, row in enumerate(matrix):\n",
    "        group = {key: [] for key in data.keys()}\n",
    "        for j, (key, value_list) in enumerate(data_copy.items()): \n",
    "            num_items = row[j]\n",
    "            group[key], data_copy[key] = value_list[:num_items], value_list[num_items:]\n",
    "\n",
    "        with open(f'lans/splits_uneven/group_{i}.json', 'w') as f:\n",
    "            json.dump(group, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in all_data.values():\n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846e4f6b6cdcd05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T11:58:17.907112500Z",
     "start_time": "2023-12-23T11:57:52.889248900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Divide the data according to the matrix\n",
    "divide_data_according_to_matrix(all_data, balanced_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c0456",
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in all_data.values():\n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f55dd2dd9de64",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(all_data)\n",
    "print(balanced_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97eeb372f753ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, value in all_data.items():\n",
    "    print(f'{key}: {len(value)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a949a3d81aaf8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 独享标签划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b713158d73068",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T12:46:13.714616600Z",
     "start_time": "2023-12-23T12:46:13.711229900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_counts = {\n",
    "    \"go\": 167288,\n",
    "    \"java\": 164923,\n",
    "    \"javascript\": 58025,\n",
    "    \"php\": 241241,\n",
    "    \"python\": 251820,\n",
    "    \"ruby\": 24927\n",
    "}\n",
    "\n",
    "sum_data = sum(data_counts.values())\n",
    "\n",
    "total_data = sum(data_counts.values()) - data_counts[\"javascript\"]\n",
    "\n",
    "data_per_set = sum_data // 10\n",
    "\n",
    "language_ratios = {lang: data_counts[lang] / total_data for lang in data_counts if lang != \"javascript\"}\n",
    "\n",
    "subsets = {f\"subset_{i}\": {} for i in range(0, 9)}\n",
    "\n",
    "for subset_name in subsets:\n",
    "    if \"subset_0\" in subset_name:\n",
    "        for lang, ratio in language_ratios.items():\n",
    "            subsets[subset_name][lang] = int((data_per_set - data_counts[\"javascript\"]) * ratio)\n",
    "        subsets[subset_name][\"javascript\"] = data_counts[\"javascript\"]\n",
    "    else:\n",
    "        for lang, data_per_subset in language_ratios.items():\n",
    "            subsets[subset_name][lang] = int(data_per_set * data_per_subset)\n",
    "\n",
    "for subset_name, subset_data in subsets.items():\n",
    "    print(f\"{subset_name}: {subset_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73199c139b944e9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T12:46:47.592918Z",
     "start_time": "2023-12-23T12:46:47.495996800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def divide_data_according_to_subsets(data, subsets):\n",
    "    \"\"\"\n",
    "    Divide the data in 'data' dictionary according to the distribution specified in 'subsets'.\n",
    "\n",
    "    :param data: Dictionary containing lists of data for each key.\n",
    "    :param subsets: Dictionary specifying how to divide the data for each key.\n",
    "    :return: A list of dictionaries, each dictionary represents a divided group of data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a directory to store the groups\n",
    "    os.makedirs('lans/splits_special_tags', exist_ok=True)\n",
    "\n",
    "    # Initialize a list to store the divided groups\n",
    "    divided_groups = []\n",
    "\n",
    "    # Iterate over each subset definition\n",
    "    for subset_name, subset_data in subsets.items():\n",
    "        group = {key: [] for key in data.keys()}\n",
    "        \n",
    "        # Iterate over each key (language) and its value (number of items to include)\n",
    "        for key, num_items in subset_data.items():\n",
    "            # Use the specified number of items to slice the data list\n",
    "            group[key] = data[key][:num_items]\n",
    "            data[key] = data[key][num_items:]\n",
    "\n",
    "        # Append the group to the list of divided groups\n",
    "        divided_groups.append(group)\n",
    "\n",
    "    # Optionally, save the divided groups to JSON files\n",
    "    for i, group in enumerate(divided_groups):\n",
    "        with open(f'lans/splits_special_tags/group_{i}.json', 'w') as f:\n",
    "            json.dump(group, f, indent=4)\n",
    "    \n",
    "    return divided_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2fbad57aafb0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T12:47:09.231685500Z",
     "start_time": "2023-12-23T12:47:09.139496500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, value in all_data.items():\n",
    "    print(f'{key}: {len(value)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68da40581c1e142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T12:48:26.838471600Z",
     "start_time": "2023-12-23T12:47:25.690900400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Divide the data according to the matrix\n",
    "divide_data_according_to_subsets(all_data, subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9ddbee93055c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
